{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sattwikjana/Sattwikjana/blob/main/BRAIN%20TUMOR%20CLASSIFICATION%20USINGCONVOLUTIONAL%20NEURAL%20NETWORK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsyAXLXrhfjV",
        "outputId": "5ea3142f-7007-4fc6-87c9-c43766b90012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/1608934/2645886/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240501%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240501T141904Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4c8eabeb8da0addc30ad30ac31d2bb768b205e8a256cd48fcb0358a6f6d627d05f5f7e80733ea1848e66b8c0381088547789ecd118172de339ef924b732585e5a37773348bf8b14d1ae1451b9e821e582f2a26cf7614e60e90fc63184302e75017ec4580c0539f0a2a5d52fd9c8eaeec5fa06541a00e86eb5d7fbc0ea09f8079bcb6c6cc070abcd6b2c68a42c3a900ebcf9696cafb10865779653bec3a22932c04bd5565b0319b998d74876d47084de12f063b52a02f095f4b89d384de92d6fc3ea966fa5ee7ed7ab8a5ecdc7fc1aa9cc885451b3f779eb7722bbb1cfa55e55fe703c7a82c5b18a66543ab8a75f769812fc5b59b7879aa1c63c31a4a97f03764 to path /kaggle/input/brain-tumor-mri-dataset\n",
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/3505991/6145850/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240501%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240501T141904Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=88c32f97c0cb1a970413ad3ebb304db806caad6395af8797e18ee260fa2c283d3c6f4201e8078a30d257e73f7a8511bd154ff826c27bbaa43ae187537500ba9c8d9ba6d56ccbed608040e8cea096311e1a3afc1ccf3776714c026359ccd86cb19851f0e493a891f22dfde22c82683663ee3e903b9f5fe1cd950e8ab5af68653fa939c1a11ca35f78870b16b4bf164e0b190349e36297aedf89cca2601f19dd8e8192572127afe2edfa9591ab8e3417e1c1f32ba882f4ceb41f91e398851fc69b2f70747998c802917c83325dc792f4ac10407969d0ac26b68204ba16b184f17c33765f89f358da064e98b5f59fcb31f80ab2706866fb7818dbf39a1cd9811f53 to path /kaggle/input/brain-tumors-dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'brain-tumor-mri-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1608934%2F2645886%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240501%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240501T141904Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4c8eabeb8da0addc30ad30ac31d2bb768b205e8a256cd48fcb0358a6f6d627d05f5f7e80733ea1848e66b8c0381088547789ecd118172de339ef924b732585e5a37773348bf8b14d1ae1451b9e821e582f2a26cf7614e60e90fc63184302e75017ec4580c0539f0a2a5d52fd9c8eaeec5fa06541a00e86eb5d7fbc0ea09f8079bcb6c6cc070abcd6b2c68a42c3a900ebcf9696cafb10865779653bec3a22932c04bd5565b0319b998d74876d47084de12f063b52a02f095f4b89d384de92d6fc3ea966fa5ee7ed7ab8a5ecdc7fc1aa9cc885451b3f779eb7722bbb1cfa55e55fe703c7a82c5b18a66543ab8a75f769812fc5b59b7879aa1c63c31a4a97f03764,brain-tumors-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3505991%2F6145850%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240501%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240501T141904Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D88c32f97c0cb1a970413ad3ebb304db806caad6395af8797e18ee260fa2c283d3c6f4201e8078a30d257e73f7a8511bd154ff826c27bbaa43ae187537500ba9c8d9ba6d56ccbed608040e8cea096311e1a3afc1ccf3776714c026359ccd86cb19851f0e493a891f22dfde22c82683663ee3e903b9f5fe1cd950e8ab5af68653fa939c1a11ca35f78870b16b4bf164e0b190349e36297aedf89cca2601f19dd8e8192572127afe2edfa9591ab8e3417e1c1f32ba882f4ceb41f91e398851fc69b2f70747998c802917c83325dc792f4ac10407969d0ac26b68204ba16b184f17c33765f89f358da064e98b5f59fcb31f80ab2706866fb7818dbf39a1cd9811f53'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eXG-Wp3Ko82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea58bdc-fad5-469e-aa79-395860aa6cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0\n",
            "Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.37.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.5.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.3.0.75 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (9.3.0.75)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.3.61 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (11.2.3.61)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.6.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (10.3.6.82)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.3.83 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.1.3)\n",
            "Collecting nvidia-nccl-cu12==2.23.4 (from tensorflow[and-cuda])\n",
            "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow[and-cuda]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow[and-cuda]) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow[and-cuda]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow[and-cuda]) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow[and-cuda]) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow[and-cuda]) (0.1.2)\n",
            "Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.23.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.23.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow[and-cuda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbXle-dnKp7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a8b2a2-a2a4-4874-a70e-7fbf220250e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version: 2.19.0\n",
            "GPU Available: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "# General Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Neural Network imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Image augmentation importrs\n",
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import RandomRotation\n",
        "from tensorflow.keras.layers import RandomContrast\n",
        "from tensorflow.keras.layers import RandomZoom\n",
        "from tensorflow.keras.layers import RandomFlip\n",
        "from tensorflow.keras.layers import RandomTranslation\n",
        "\n",
        "# Training Model callbacks\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Check if GPU is available\n",
        "print(f'Tensorflow Version: {tf.__version__}')\n",
        "#print(\"GPU Available:\", tf.config.list_physical_devices('GPU')[0])\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"GPU Available:\", gpus[0])\n",
        "else:\n",
        "    print(\"No GPUs available\")\n",
        "SEED = 111\n",
        "\n",
        "# Data Visualization updates\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.rcParams[\"figure.figsize\"] = (16, 10)\n",
        "plt.rcParams.update({'font.size': 14})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Uh2PIj5KtKi"
      },
      "outputs": [],
      "source": [
        "def get_data_labels(directory, shuffle=True, random_state=0):\n",
        "    from sklearn.utils import shuffle\n",
        "    data_path = []\n",
        "    data_index = []\n",
        "    label_dict = {label: index for index, label in enumerate(sorted(os.listdir(directory)))}\n",
        "\n",
        "    for label, index in label_dict.items():\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for image in os.listdir(label_dir):\n",
        "            image_path = os.path.join(label_dir, image)\n",
        "            data_path.append(image_path)\n",
        "            data_index.append(index)\n",
        "\n",
        "    if shuffle:\n",
        "        data_path, data_index = shuffle(data_path, data_index, random_state=random_state)\n",
        "\n",
        "    return data_path, data_index\n",
        "\n",
        "def parse_function(filename, label, image_size, n_channels):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_string, n_channels)\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    return image, label\n",
        "\n",
        "def get_dataset(paths, labels, image_size, n_channels=1, num_classes=4, batch_size=32):\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    image_label_ds = path_ds.map(lambda path, label: parse_function(path, label, image_size, n_channels),\n",
        "                                 num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return image_label_ds.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqrIkeJ-KwnO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "e302be81-47ae-4280-9ca6-fe5b249467cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/brain-tumor-mri-dataset/Training'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-314768297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Getting data labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mUSER_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/brain-tumor-mri-dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSER_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSER_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Testing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-965318154.py\u001b[0m in \u001b[0;36mget_data_labels\u001b[0;34m(directory, shuffle, random_state)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlabel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/brain-tumor-mri-dataset/Training'"
          ]
        }
      ],
      "source": [
        "# Getting data labels\n",
        "USER_PATH = \"/kaggle/input/brain-tumor-mri-dataset\"\n",
        "train_paths, train_index = get_data_labels(USER_PATH + '/Training', random_state=SEED)\n",
        "test_paths, test_index = get_data_labels(USER_PATH + '/Testing', random_state=SEED)\n",
        "\n",
        "# Printing traing and testing sample sizes\n",
        "print('Training')\n",
        "print(f'Number of Paths: {len(train_paths)}')\n",
        "print(f'Number of Labels: {len(train_index)}')\n",
        "print('\\nTesting')\n",
        "print(f'Number of Paths: {len(test_paths)}')\n",
        "print(f'Number of Labels: {len(test_index)}')\n",
        "\n",
        "# Prepare datasets with 4 classes and grayscale\n",
        "batch_size = 32\n",
        "image_dim = (168, 168)\n",
        "train_ds = get_dataset(train_paths, train_index, image_dim, n_channels=1, num_classes=4, batch_size=batch_size)\n",
        "test_ds = get_dataset(test_paths, test_index, image_dim, n_channels=1, num_classes=4, batch_size=batch_size)\n",
        "\n",
        "# Output to show datasets\n",
        "print(f\"\\nTraining dataset: {train_ds}\")\n",
        "print(f\"\\nTesting dataset: {test_ds}\")\n",
        "\n",
        "# Class mappings\n",
        "class_mappings = {'Glioma': 0, 'Meninigioma': 1, 'Notumor': 2, 'Pituitary': 3}\n",
        "inv_class_mappings = {v: k for k, v in class_mappings.items()}\n",
        "class_names = list(class_mappings.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4un3E5iK0wl"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(ncols=3, figsize=(20, 14))\n",
        "\n",
        "# Plotting training data types\n",
        "class_counts = [len([x for x in train_index if x == label]) for label in set(train_index)]\n",
        "ax[0].set_title('Training Data')\n",
        "ax[0].pie(\n",
        "    class_counts,\n",
        "    labels=[label for label in class_names],\n",
        "    colors=['#FAC500','#0BFA00', '#0066FA','#FA0000'],\n",
        "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum(class_counts) / 100),\n",
        "    explode=(0.01, 0.01, 0.1, 0.01),\n",
        "    textprops={'fontsize': 20}\n",
        ")\n",
        "\n",
        "# Plotting distribution of train test split\n",
        "ax[1].set_title('Train Test Split')\n",
        "ax[1].pie(\n",
        "    [len(train_index), len(test_index)],\n",
        "    labels=['Train','Test'],\n",
        "    colors=['darkcyan', 'orange'],\n",
        "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum([len(train_index), len(test_index)]) / 100),\n",
        "    explode=(0.1, 0),\n",
        "    startangle=85,\n",
        "    textprops={'fontsize': 20}\n",
        ")\n",
        "\n",
        "# Plotting testing data types\n",
        "class_counts = [len([x for x in test_index if x == label]) for label in set(test_index)]\n",
        "ax[2].set_title('Testing Data')\n",
        "ax[2].pie(\n",
        "    class_counts,\n",
        "    labels=[label for label in class_names],\n",
        "    colors=['#FAC500', '#0BFA00', '#0066FA', '#FA0000'],\n",
        "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum(class_counts) / 100),\n",
        "    explode=(0.01, 0.01, 0.1, 0.01),\n",
        "    textprops={'fontsize': 20}\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sheWrPBMK3Kx"
      },
      "outputs": [],
      "source": [
        "# Function to display a list of images based on the given index\n",
        "def show_images(paths, label_paths, class_mappings, index_list=range(10), im_size=250, figsize=(12, 8)):\n",
        "\n",
        "    num_images = len(index_list)\n",
        "    num_rows = (num_images + 3) // 4\n",
        "    index_to_class = {v: k for k, v in class_mappings.items()}\n",
        "    _, ax = plt.subplots(nrows=num_rows, ncols=4, figsize=figsize)\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    for i, index in enumerate(index_list):\n",
        "        if i >= num_images:\n",
        "            break\n",
        "        image = load_img(paths[index], target_size=(im_size, im_size), color_mode='grayscale')\n",
        "        ax[i].imshow(image, cmap='Greys_r')\n",
        "        class_name = index_to_class[label_paths[index]]\n",
        "        ax[i].set_title(f'{index}: {class_name}')\n",
        "        ax[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOsTzBX6K3zr"
      },
      "outputs": [],
      "source": [
        "# Four images from different angles\n",
        "show_images(train_paths, train_index, class_mappings, im_size=350, figsize=(13,10),\n",
        "            index_list=range(100, 112))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oldeuO1K99o"
      },
      "outputs": [],
      "source": [
        "# Dta augmentation sequential model\n",
        "data_augmentation = Sequential([\n",
        "    # RandomFlip(\"horizontal_and_vertical\"),\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.02, fill_mode='constant'),\n",
        "    RandomContrast(0.1),\n",
        "    RandomZoom(height_factor=0.01, width_factor=0.05),\n",
        "    RandomTranslation(height_factor=0.0015, width_factor=0.0015, fill_mode='constant'),\n",
        "])\n",
        "\n",
        "# Training augmentation and nornalization\n",
        "def preprocess_train(image, label):\n",
        "    # Apply data augmentation and Normalize\n",
        "    image = data_augmentation(image) / 255.0\n",
        "    return image, label\n",
        "\n",
        "# For test dataset only appying normalization\n",
        "def preprocess_test(image, label):\n",
        "    return image / 255.0, label\n",
        "\n",
        "# Apply transformation to training and testing datasets\n",
        "train_ds_preprocessed = train_ds.map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds_preprocessed = test_ds.map(preprocess_test, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXIsLsqnLCIM"
      },
      "outputs": [],
      "source": [
        "# Function to display augmented images\n",
        "def plot_augmented_images(dataset, shape, class_mappings, figsize=(15, 6)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    index_to_class = {v: k for k, v in class_mappings.items()}\n",
        "    for images, label in dataset.take(1):\n",
        "        i = 0\n",
        "        for i in range(shape[0]*shape[1]):\n",
        "            ax = plt.subplot(shape[0], shape[1], i + 1)\n",
        "            plt.imshow(images[i].numpy().squeeze(), cmap='gray')\n",
        "            plt.title(index_to_class[label.numpy()[i]])\n",
        "            plt.axis(\"off\")\n",
        "            i += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja6jeLqcLHwu"
      },
      "outputs": [],
      "source": [
        "# Displaying augmented images\n",
        "plot_augmented_images(train_ds_preprocessed, shape=(2, 6), class_mappings=class_mappings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvNNIl22LJ8_"
      },
      "outputs": [],
      "source": [
        "# Classes and Image shape: height, width, grayscale\n",
        "num_classes = len(class_mappings.keys())\n",
        "image_shape = (image_dim[0], image_dim[1], 1)\n",
        "\n",
        "# Training epochs and batch size\n",
        "epochs = 50\n",
        "print(f'Number of Classes: {num_classes}')\n",
        "print(f'Image shape: {image_shape}')\n",
        "print(f'Epochs: {epochs}')\n",
        "print(f'Batch size: {batch_size}')\n",
        "\n",
        "def encode_labels(image, label):\n",
        "    return image, tf.one_hot(label, depth=num_classes)\n",
        "\n",
        "train_ds_preprocessed = train_ds_preprocessed.map(encode_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds_preprocessed = test_ds_preprocessed.map(encode_labels, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLt-eQH9MGJL"
      },
      "outputs": [],
      "source": [
        "# Building model\n",
        "model = Sequential([\n",
        "    # Input tensor shape\n",
        "    Input(shape=image_shape),\n",
        "\n",
        "    # Convolutional layer 1\n",
        "    Conv2D(64, (5, 5), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(3, 3)),\n",
        "\n",
        "    # Convolutional layer 2\n",
        "    Conv2D(64, (5, 5), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(3, 3)),\n",
        "\n",
        "    # Convolutional layer 3\n",
        "    Conv2D(128, (4, 4), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Convolutional layer 4\n",
        "    Conv2D(128, (4, 4), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "\n",
        "    # Dense layers\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Compilng model with Adam optimizer\n",
        "optimizer = Adam(learning_rate=0.001, beta_1=0.85, beta_2=0.9925)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdKPrZ00hfjb"
      },
      "outputs": [],
      "source": [
        "# Custom callback for reducing learning rate at accuracy values\n",
        "class ReduceLROnMultipleAccuracies(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, thresholds, factor, monitor='val_accuracy', verbose=1):\n",
        "        super(ReduceLROnMultipleAccuracies, self).__init__()\n",
        "        self.thresholds = thresholds  # List of accuracy thresholds\n",
        "        self.factor = factor  # Factor to reduce the learning rate\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.thresholds_reached = [False] * len(thresholds)  # Track each threshold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_accuracy = logs.get(self.monitor)\n",
        "        for i, threshold in enumerate(self.thresholds):\n",
        "            if current_accuracy >= threshold and not self.thresholds_reached[i]:\n",
        "                optimizer = self.model.optimizer\n",
        "                old_lr = optimizer.learning_rate.numpy()\n",
        "                new_lr = old_lr * self.factor\n",
        "                optimizer.learning_rate.assign(new_lr)\n",
        "                self.thresholds_reached[i] = True  # Mark this threshold as reached\n",
        "                if self.verbose > 0:\n",
        "                    print(f\"\\nEpoch {epoch+1}: {self.monitor} reached {threshold}. Reducing learning rate from {old_lr} to {new_lr}.\")\n",
        "\n",
        "# Try a custom callback\n",
        "thresholds = [0.96, 0.99, 0.9935]\n",
        "lr_callback = ReduceLROnMultipleAccuracies(thresholds=thresholds, factor=0.75, monitor='val_accuracy', verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUsVQTQ5hfjb"
      },
      "outputs": [],
      "source": [
        "  # Callbacks for improved covergence of gradient and best test accuracy\n",
        "model_rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, min_lr=1e-4, patience=4, verbose=False)\n",
        "model_mc = ModelCheckpoint('model.keras', monitor='val_accuracy', mode='max', save_best_only=True, verbose=False)\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    train_ds_preprocessed,\n",
        "    epochs=10,\n",
        "    validation_data=test_ds_preprocessed,\n",
        "    callbacks=[model_rlr, model_mc],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAJNXUNAMpuC"
      },
      "outputs": [],
      "source": [
        "# Loading saved model\n",
        "model = load_model('model.keras')\n",
        "\n",
        "# Evaluate model and test data accuracy\n",
        "test_loss, test_acc = model.evaluate(test_ds_preprocessed)\n",
        "print(f\"Test accuracy: {test_acc*100:0.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlyPzYYIMqil"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(ncols=2, figsize=(15, 6))\n",
        "\n",
        "# Plotting training and validation accuracy over epochs\n",
        "ax[0].plot(history.history['accuracy'], marker='o', linestyle='-', color='blue')\n",
        "ax[0].plot(history.history['val_accuracy'], marker='o', linestyle='-', color='orange')\n",
        "ax[0].set_title('Model Accuracy')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].legend(['Train', 'Validation'], loc='lower right')\n",
        "ax[0].grid(alpha=0.2)\n",
        "\n",
        "# Plotting training and validation loss over epochs\n",
        "ax[1].plot(history.history['loss'], marker='o', linestyle='-', color='blue')\n",
        "ax[1].plot(history.history['val_loss'], marker='o', linestyle='-', color='orange')\n",
        "ax[1].set_title('Model Loss')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].legend(['Train', 'Validation'], loc='upper right')\n",
        "ax[1].grid(alpha=0.2)\n",
        "\n",
        "# Highlight lowest validation accuracy\n",
        "min_val_acc_epoch = np.argmax(history.history['val_accuracy'])\n",
        "min_val_acc = np.max(history.history['val_accuracy'])\n",
        "ax[0].plot(min_val_acc_epoch, min_val_acc, 'ro', markersize=15, alpha=0.5)\n",
        "ax[0].annotate(f'Lowest\\n{min_val_acc:.4f}', xy=(min_val_acc_epoch, min_val_acc),\n",
        "               xytext=(min_val_acc_epoch - 100, min_val_acc - 100), textcoords='offset points',\n",
        "               arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=.2'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ccWdMbdMsqC"
      },
      "outputs": [],
      "source": [
        "# Using test data for true and preductions\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate over dataset to collect predictions and true labels\n",
        "# Unbatch to get sample-wise prediction\n",
        "for images, labels in test_ds_preprocessed.unbatch():\n",
        "    # Store true labels (Convert one-hot to index)\n",
        "    true_label = np.argmax(labels.numpy())\n",
        "    true_labels.append(true_label)\n",
        "\n",
        "    # Get model prediction (Predict expects batch dimension)\n",
        "    pred = model.predict(tf.expand_dims(images, 0), verbose=False)\n",
        "    predicted_label = np.argmax(pred)\n",
        "    predicted_labels.append(predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtQbq7-3MuwC"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(true_labels, predicted_labels, class_mappings, metrics=False, cmap='Blues'):\n",
        "    # Compute  confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, cbar=False)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "\n",
        "    # Mapping of indices to class names in class_mappings\n",
        "    plt.xticks(ticks=np.arange(num_classes) + 0.5, labels=class_mappings.keys(), ha='center')\n",
        "    plt.yticks(ticks=np.arange(num_classes) + 0.5, labels=class_mappings.keys(), va='center')\n",
        "    plt.show()\n",
        "\n",
        "    if metrics:\n",
        "        # Precision, Recall, and F1-Score for each class & Overall accuracy\n",
        "        precision = np.diag(cm) / np.sum(cm, axis=0)\n",
        "        recall = np.diag(cm) / np.sum(cm, axis=1)\n",
        "        f1_scores = 2 * precision * recall / (precision + recall)\n",
        "        accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "        print(\"Class-wise metrics:\")\n",
        "        for i in range(len(class_mappings)):\n",
        "            class_name = list(class_mappings.keys())[i]\n",
        "            print(f\"\\033[94mClass: {class_name}\\033[0m\")\n",
        "            print(f\"Precision: {precision[i]:.4f}\")\n",
        "            print(f\"Recall: {recall[i]:.4f}\")\n",
        "            print(f\"F1-Score: {f1_scores[i]:.4f}\\n\")\n",
        "\n",
        "        print(f\"\\033[92mOverall Accuracy: {accuracy:.4f}\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c3qcbaHMwp7"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and netrics from predictions\n",
        "plot_confusion_matrix(true_labels,\n",
        "                      predicted_labels,\n",
        "                      class_mappings,\n",
        "                      metrics=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agz7svrtMyf-"
      },
      "outputs": [],
      "source": [
        "def plot_sample_predictions(model, dataset, index_to_class, num_samples=9, figsize=(13, 12)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    num_rows = num_cols = int(np.sqrt(num_samples))\n",
        "\n",
        "    iterator = iter(dataset.unbatch())\n",
        "\n",
        "    for i in range(1, num_samples + 1):\n",
        "        image, true_label = next(iterator)\n",
        "        image_batch = tf.expand_dims(image, 0)\n",
        "        predictions = model.predict(image_batch, verbose=False)\n",
        "        predicted_label = np.argmax(predictions, axis=1)[0]\n",
        "\n",
        "        true_class_index = np.argmax(true_label.numpy())\n",
        "        true_class = index_to_class[true_class_index]\n",
        "        predicted_class = index_to_class[predicted_label]\n",
        "\n",
        "        # Determine title color based on prediction accuracy\n",
        "        title_color = 'green' if true_class_index == predicted_label else 'red'\n",
        "\n",
        "        plt.subplot(num_rows, num_cols, i)\n",
        "        plt.imshow(image.numpy().squeeze(), cmap='gray')\n",
        "        plt.title(f\"True: {true_class}\\nPred: {predicted_class}\", color=title_color)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NatBxGQeM0r7"
      },
      "outputs": [],
      "source": [
        "# Plottinng samples with predictions\n",
        "plot_sample_predictions(model=model,\n",
        "                        dataset=test_ds_preprocessed,\n",
        "                        index_to_class=inv_class_mappings,\n",
        "                        num_samples=9,\n",
        "                        figsize=(10, 11.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HI00YH4M2YV"
      },
      "outputs": [],
      "source": [
        "def plot_misclassified_samples(model, dataset, index_to_class, figsize=(10, 10)):\n",
        "    misclassified_images = []\n",
        "    misclassified_labels = []\n",
        "    misclassified_predictions = []\n",
        "\n",
        "    # Iterate over dataset to collect misclassified images\n",
        "    for image, true_label in dataset.unbatch():\n",
        "        image_batch = tf.expand_dims(image, 0)\n",
        "        predictions = model.predict(image_batch, verbose=False)\n",
        "        predicted_label = np.argmax(predictions, axis=1)[0]\n",
        "        true_class_index = np.argmax(true_label.numpy())\n",
        "\n",
        "        if true_class_index != predicted_label:\n",
        "            misclassified_images.append(image.numpy().squeeze())\n",
        "            misclassified_labels.append(index_to_class[true_class_index])\n",
        "            misclassified_predictions.append(index_to_class[predicted_label])\n",
        "\n",
        "    # Determine number of rows and columns for subplot\n",
        "    num_misclassified = len(misclassified_images)\n",
        "    cols = int(np.sqrt(num_misclassified)) + 1\n",
        "    rows = num_misclassified // cols + (num_misclassified % cols > 0)\n",
        "\n",
        "    # Plotting misclassified images\n",
        "    miss_classified_zip = zip(misclassified_images, misclassified_labels, misclassified_predictions)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i, (image, true_label, predicted_label) in enumerate(miss_classified_zip):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.title(f\"True: {true_label}\\nPred: {predicted_label}\", color='red')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xOjkZIPM4za"
      },
      "outputs": [],
      "source": [
        "# Plotting misclassified images\n",
        "plot_misclassified_samples(model=model,\n",
        "                           dataset=test_ds_preprocessed,\n",
        "                           index_to_class=inv_class_mappings,\n",
        "                           figsize=(10, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LszQR_GYM6k6"
      },
      "outputs": [],
      "source": [
        "# Function to load and preprocess an image\n",
        "def load_and_preprocess_image(image_path, image_shape=(168, 168)):\n",
        "    img = image.load_img(image_path, target_size=image_shape, color_mode='grayscale')\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add the batch dimension\n",
        "    return img_array\n",
        "\n",
        "# Function to display a row of images with predictions\n",
        "def display_images_and_predictions(image_paths, predictions, true_labels, figsize=(20, 5)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i, (image_path, prediction, true_label) in enumerate(zip(image_paths, predictions, true_labels)):\n",
        "        ax = plt.subplot(1, len(image_paths), i + 1)\n",
        "        img_array = load_and_preprocess_image(image_path)\n",
        "        img_array = np.squeeze(img_array)\n",
        "        plt.imshow(img_array, cmap='gray')\n",
        "        title_color = 'green' if prediction == true_label else 'red'\n",
        "        plt.title(f'True Label: {true_label}\\nPred: {prediction}', color=title_color)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15U_Jyp_M8V6"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the images\n",
        "normal_image_path = '/kaggle/input/brain-tumors-dataset/Data/Normal/N_103.jpg'\n",
        "glioma_image_path = '/kaggle/input/brain-tumors-dataset/Data/Tumor/glioma_tumor/G_10.jpg'\n",
        "meningioma_image_path = '/kaggle/input/brain-tumors-dataset/Data/Tumor/meningioma_tumor/M_102_HF_.jpg'\n",
        "pituitary_tumor_path = '/kaggle/input/brain-tumors-dataset/Data/Tumor/pituitary_tumor/P_100_HF_.jpg'\n",
        "\n",
        "# Image paths\n",
        "image_paths = [\n",
        "    normal_image_path,\n",
        "    glioma_image_path,\n",
        "    meningioma_image_path,\n",
        "    pituitary_tumor_path\n",
        "]\n",
        "\n",
        "# True labels for images\n",
        "true_labels = ['Notumor', 'Glioma', 'Meninigioma', 'Pituitary']\n",
        "\n",
        "# Load and preprocess images, then make predictions\n",
        "images = [load_and_preprocess_image(path) for path in image_paths]\n",
        "predictions = [model.predict(image) for image in images]\n",
        "\n",
        "# Determine the predicted labels\n",
        "predicted_labels = [inv_class_mappings[np.argmax(one_hot)] for one_hot in predictions]\n",
        "\n",
        "# Output the predictions\n",
        "print(f'Class Mappings: {class_mappings}')\n",
        "print(\"\\nNormal Image Prediction:\", np.round(predictions[0], 3)[0])\n",
        "print(\"Glioma Image Prediction:\", np.round(predictions[1], 3)[0])\n",
        "print(\"Meningioma Image Prediction:\", np.round(predictions[2], 3)[0])\n",
        "print(\"Pituitary Image Prediction:\", np.round(predictions[3], 3)[0])\n",
        "\n",
        "# Display images with predictions\n",
        "display_images_and_predictions(image_paths, predicted_labels, true_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}